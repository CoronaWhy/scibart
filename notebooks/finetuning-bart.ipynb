{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning BART for abstractive text summarisation with fastai2\n",
    "\n",
    "A great thing about working in NLP at the moment is being able to park a hard problem for a few weeks and discovering the community making massive amounts of progress on your behalf. I used to be overwhelmed by the challenge of just training a summarisation model to generate plausible looking text without burning through tonnes of cash on GPUs. Then [BertExtAbs](../finetuning-bertsumextabs) came along and solved that problem. Unfortunately, it still gernerated incoherent sentences sometimes and had a habit of confusing entities in an article. You certainly couldn't trust it to convey the facts of an article reliably.\n",
    "\n",
    "Enter BART (Bidirectional and Auto-Regressive Transformers). Here we have a model that generates staggeringly good summaries and has a wonderful implementation from Sam Shleifer at HuggingFace. It's still a work in progress, but after digging around in the Transformers pull requests and with help from [Morgan McGuire's FastHugs notebook](https://github.com/morganmcg1/fasthugs) I have put together this notebook for fine-tuning BART and generating summaries. Feedback welcome!\n",
    "\n",
    "I should mention that this a big model requiring big inputs. For fine-tuning I've been able to get a batch size of 4 and a maximum sequence length of 512 on an AWS P3.2xlarge (~£4 an hour).\n",
    "\n",
    "We begin with a bunch of imports and an args object for storing variables we will need. We'll be finetuning the model on the Curation Corpus of abstractive text summaries. We load it into a dataframe using Pandas. For more information about how to access this dataset for your own purposes please see our [article introducing the dataset](https://medium.com/curation-corporation/teaching-an-ai-to-abstract-a-new-dataset-for-abstractive-auto-summarisation-5227f546caa8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastai2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/2b/9e0f6d3d928105dec034727ed723c33340f71a77ca253b67bbec99eacd3f/fastai2-0.0.16-py3-none-any.whl (184kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 761kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (1.0.3)\n",
      "Collecting transformers\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/site-packages (from fastai2) (2.2.4)\n",
      "Collecting fastcore\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/b5/aed836ce5b16ea1088a5d1a41d400bc051abf90bbef58bb74d8fd01a76af/fastcore-0.1.16-py3-none-any.whl\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/site-packages (from fastai2) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (from fastai2) (3.13)\n",
      "Requirement already satisfied: fastprogress>=0.1.22 in /usr/local/lib/python3.7/site-packages (from fastai2) (0.2.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from fastai2) (2.21.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/site-packages (from fastai2) (7.0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/site-packages (from fastai2) (0.21.3)\n",
      "Collecting torchvision>=0.5\n",
      "  Using cached https://files.pythonhosted.org/packages/02/c8/26ff0db66e6dd30a3ed2bfbceae9744359ae4cbb48864c70121a41c21ca5/torchvision-0.5.0-cp37-cp37m-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/site-packages (from fastai2) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas) (2018.5)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from pandas) (1.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas) (2.7.3)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/d7/a3882b2d36991f613b749fc5e305cccc345ec9d6ab0621ad7e7bf1be8691/tokenizers-0.5.2.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 1.7MB/s eta 0:00:011\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.7/site-packages (from transformers) (1.9.111)\n",
      "Collecting sentencepiece\n",
      "  Using cached https://files.pythonhosted.org/packages/e6/56/2e6cfc364c4760b85adab40cb38d91e7ce67d6b2745a2e1aa1497c776fe1/sentencepiece-0.1.85-cp37-cp37m-macosx_10_6_x86_64.whl\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached https://files.pythonhosted.org/packages/4c/e7/eee73c42c1193fecc0e91361a163cbb8dfbea62c3db7618ad986e5b43a14/regex-2020.4.4.tar.gz\n",
      "Collecting sacremoses\n",
      "  Using cached https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/site-packages (from transformers) (4.41.1)\n",
      "Collecting filelock\n",
      "  Using cached https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (0.6.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (7.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (42.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (3.0.2)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->fastai2) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->fastai2) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->fastai2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->fastai2) (2019.3.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn->fastai2) (0.13.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from torchvision>=0.5->fastai2) (1.13.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->fastai2) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->fastai2) (2.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib->fastai2) (0.10.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3->transformers) (0.9.4)\n",
      "Collecting s3transfer<0.3.0,>=0.2.0\n",
      "  Using cached https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl\n",
      "Collecting botocore<1.13.0,>=1.12.111\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/7b/88f10115b4748f86be6b7b1d8761ba5023fccf6e6cbe762e368f63eddcf9/botocore-1.12.253-py2.py3-none-any.whl (5.7MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8MB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Using cached https://files.pythonhosted.org/packages/dd/c0/4d8f43a9b16e289f36478422031b8a63b54b6ac3b1ba605d602f10dd54d6/click-7.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai2) (1.6.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.111->boto3->transformers) (0.14)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai2) (3.1.0)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tokenizers: filename=tokenizers-0.5.2-cp37-cp37m-macosx_10_13_x86_64.whl size=1120025 sha256=762a110c61d953cba192c51c572f68f86b9b2521fa1b75ad6baa91705b403c47\n",
      "  Stored in directory: /Users/alexwalther/Library/Caches/pip/wheels/c3/2c/66/45b98211d5dd54f35bbe33994a672cfc1a819d5ce84a0a534a\n",
      "Successfully built tokenizers\n",
      "Building wheels for collected packages: regex, sacremoses\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2020.4.4-cp37-cp37m-macosx_10_13_x86_64.whl size=284994 sha256=213efe140fbb78f4d357401cfdfa7744395d9602d74aeedd33dc80e3ed395dff\n",
      "  Stored in directory: /Users/alexwalther/Library/Caches/pip/wheels/e6/9b/ae/2972da29cc7759b71dee015813b7c6931917d6a51e64ed5e79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp37-none-any.whl size=884628 sha256=ebd85f2b1a9bc09725590000d8db70de398a07fcab0969157c538dcc62eda1e8\n",
      "  Stored in directory: /Users/alexwalther/Library/Caches/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built regex sacremoses\n",
      "\u001b[31mERROR: awscli 1.18.39 has requirement botocore==1.15.39, but you'll have botocore 1.12.253 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: awscli 1.18.39 has requirement s3transfer<0.4.0,>=0.3.0, but you'll have s3transfer 0.2.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.3.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: fastcore, torchvision, fastai2, tokenizers, sentencepiece, regex, click, sacremoses, filelock, transformers, botocore, s3transfer\n",
      "  Found existing installation: botocore 1.15.39\n",
      "    Uninstalling botocore-1.15.39:\n",
      "      Successfully uninstalled botocore-1.15.39\n",
      "  Found existing installation: s3transfer 0.3.3\n",
      "    Uninstalling s3transfer-0.3.3:\n",
      "      Successfully uninstalled s3transfer-0.3.3\n",
      "Successfully installed botocore-1.12.253 click-7.1.1 fastai2-0.0.16 fastcore-0.1.16 filelock-3.0.12 regex-2020.4.4 s3transfer-0.2.1 sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 torchvision-0.5.0 transformers-2.8.0\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install fastai2 sklearn pandas transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import logging\n",
    "logging.getLogger().setLevel(100)\n",
    "from fastprogress import progress_bar\n",
    "from fastai2.basics import *\n",
    "from fastai2.data import *\n",
    "from fastai2.text.all import *\n",
    "from fastai2.callback.all import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import PreTrainedTokenizer, BartTokenizer, BartForConditionalGeneration, BartConfig \n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully we will be able to increase our batch size and/or maximum sequence lengths when some pull requests to reduce the model's memory footprint get merged into the Transformers repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "args = Namespace(\n",
    "    batch_size=4,\n",
    "    max_seq_len=512,\n",
    "    data_path=\"../data/private_dataset.file\",\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), # ('cpu'),\n",
    "    stories_folder='../data/my_own_stories',\n",
    "    subset=None,\n",
    "    test_pct=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset after scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showmult(*args):\n",
    "    return [i for i in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit: CC0 Public Domain\\n             \\n\\nUniversity of British Columbia researchers have found a cheap, sustainable way to build a solar cell using bacteria that convert light to energy.\\n                                              \\nTheir cell generated a current stronger than any previously recorded from such a device, and worked as efficiently in dim light as in bright light.\\nThis innovation could be a step toward wider adoption of solar power in places like British Columbia and parts of northern Europe where overcast skies are common. With further development, these solar cells—c...</td>\n",
       "      <td>Researchers in Canada have developed an innovative solar cell which uses bacteria to convert light into energy. The team at the University of British Columbia developed a way of genetically engineering E.coli to produce large amounts of lycopene, a natural dye which bacteria use for photosynthesis. By coating the bacteria with a semiconducting substance and incorporating it into a battery cell, they were able to achieve a current density of 0.686 milliamps per square centimetre, which they say is the highest yet achieved by a biogenic solar cell.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Genesis Motor America is launching two new online tools as part of its newly redesigned website.In conjunction with the debut of the Genesis G70, AOR Innocean took gaming technologies and\\nInstagram Stories and created two new automotive \"configurators.\"The first, a web-based Automotive Real-Time 3D Configurator, can only be found on the updated website, Genesis.com. Here, users can build a virtual vehicle while exploring inside and outside the car, activating animations that emulate real vehicle actions including\\nheadlights, sunroof, and dashboard. They can even pop the trunk open. Users...</td>\n",
       "      <td>Luxury car maker Genesis Motor America launched a redesigned website that included two interactive tools that enable users to build a virtual vehicle complete with animations that mimic functions such as opening the sunroof. One of the tools allows Instagram users to build their car on the app using Instagram Stories. Innocean lead the project handling the creative and concept design. MediaMonks was one of four other companies that participated on the project, working on the WebGL Game Engine.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When Jamie Hodari started looking for funding for his New York-based co-working startup, Industrious, in 2012, he didn’t even bother talking to venture capital firms. \\n“We were just so confident that VCs didn’t fund real estate that it wasn’t worth trying,” he said. \\nInstead, Hodari and his co-founder, Justin Stewart, went to anyone they knew — parents, siblings, aunts, uncles, friends — who had money and pitched them on investing in individual locations. While they managed to raise $8 million, it was tedious. They couldn’t sign a lease or get going on a project until they had every dime...</td>\n",
       "      <td>Real-estate tech has become hot property for VC funds. Struggling to find seed funding when it launched in 2012, coworkspace operator Industrious was forced to tap friends and family for $8m. By early 2018, two rounds later, it had raised a total of $142m and had a list of 64 funds keen to invest in the sector. Industrious is one of hundreds benefiting from the explosion of interest in real-estate venture investment. From 2012 to 2017, investment in US real-estate tech leapt from $44.7m to $5.7bn, according to Pitchbook, as VC funds exclusively devoted to the sector emerged and family real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>British Insurers have today highlighted the potential dangers of ‘autonomous ambiguity’, as vehicles with different levels of autonomy, or driverless technology, increasingly become a feature of UK roads.With important and wide-reaching changes being defined by international regulators on what Assisted and Automated systems can and can't do, the Automated Driving Insurer Group (ADIG), led by the Association of British Insurers (ABI) in collaboration with Thatcham Research, has released a white paper setting out the latest position of UK insurers.The ‘Regulating Automated Driving’ paper is ...</td>\n",
       "      <td>While they believe vehicle automation will significantly reduce accidents, British insurers, including esure, have voiced concerns over ‘autonomous ambiguity’ as vehicles with different levels of autonomy take to the roads. The insurers have called for international regulators to make clear distinctions between assisted and automated systems, and set out criteria for marketing such vehicles. It is suggested that \"intermediate automated systems\", which offer significant self-driving capability but require drivers to reclaim control of the vehicle in certain circumstances, could leave driver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  Credit: CC0 Public Domain\\n             \\n\\nUniversity of British Columbia researchers have found a cheap, sustainable way to build a solar cell using bacteria that convert light to energy.\\n                                              \\nTheir cell generated a current stronger than any previously recorded from such a device, and worked as efficiently in dim light as in bright light.\\nThis innovation could be a step toward wider adoption of solar power in places like British Columbia and parts of northern Europe where overcast skies are common. With further development, these solar cells—c...   \n",
       "1  Genesis Motor America is launching two new online tools as part of its newly redesigned website.In conjunction with the debut of the Genesis G70, AOR Innocean took gaming technologies and\\nInstagram Stories and created two new automotive \"configurators.\"The first, a web-based Automotive Real-Time 3D Configurator, can only be found on the updated website, Genesis.com. Here, users can build a virtual vehicle while exploring inside and outside the car, activating animations that emulate real vehicle actions including\\nheadlights, sunroof, and dashboard. They can even pop the trunk open. Users...   \n",
       "2  When Jamie Hodari started looking for funding for his New York-based co-working startup, Industrious, in 2012, he didn’t even bother talking to venture capital firms. \\n“We were just so confident that VCs didn’t fund real estate that it wasn’t worth trying,” he said. \\nInstead, Hodari and his co-founder, Justin Stewart, went to anyone they knew — parents, siblings, aunts, uncles, friends — who had money and pitched them on investing in individual locations. While they managed to raise $8 million, it was tedious. They couldn’t sign a lease or get going on a project until they had every dime...   \n",
       "3  British Insurers have today highlighted the potential dangers of ‘autonomous ambiguity’, as vehicles with different levels of autonomy, or driverless technology, increasingly become a feature of UK roads.With important and wide-reaching changes being defined by international regulators on what Assisted and Automated systems can and can't do, the Automated Driving Insurer Group (ADIG), led by the Association of British Insurers (ABI) in collaboration with Thatcham Research, has released a white paper setting out the latest position of UK insurers.The ‘Regulating Automated Driving’ paper is ...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   summary  \n",
       "0                                                 Researchers in Canada have developed an innovative solar cell which uses bacteria to convert light into energy. The team at the University of British Columbia developed a way of genetically engineering E.coli to produce large amounts of lycopene, a natural dye which bacteria use for photosynthesis. By coating the bacteria with a semiconducting substance and incorporating it into a battery cell, they were able to achieve a current density of 0.686 milliamps per square centimetre, which they say is the highest yet achieved by a biogenic solar cell.  \n",
       "1                                                                                                     Luxury car maker Genesis Motor America launched a redesigned website that included two interactive tools that enable users to build a virtual vehicle complete with animations that mimic functions such as opening the sunroof. One of the tools allows Instagram users to build their car on the app using Instagram Stories. Innocean lead the project handling the creative and concept design. MediaMonks was one of four other companies that participated on the project, working on the WebGL Game Engine.\\n  \n",
       "2  Real-estate tech has become hot property for VC funds. Struggling to find seed funding when it launched in 2012, coworkspace operator Industrious was forced to tap friends and family for $8m. By early 2018, two rounds later, it had raised a total of $142m and had a list of 64 funds keen to invest in the sector. Industrious is one of hundreds benefiting from the explosion of interest in real-estate venture investment. From 2012 to 2017, investment in US real-estate tech leapt from $44.7m to $5.7bn, according to Pitchbook, as VC funds exclusively devoted to the sector emerged and family real...  \n",
       "3  While they believe vehicle automation will significantly reduce accidents, British insurers, including esure, have voiced concerns over ‘autonomous ambiguity’ as vehicles with different levels of autonomy take to the roads. The insurers have called for international regulators to make clear distinctions between assisted and automated systems, and set out criteria for marketing such vehicles. It is suggested that \"intermediate automated systems\", which offer significant self-driving capability but require drivers to reclaim control of the vehicle in certain circumstances, could leave driver...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = pd.read_csv('../../curation-corpus-base.csv')\n",
    "text = pd.read_csv('../../curation-corpus-base-with-articles.csv')\n",
    "text = text[text.html != 'Exception']\n",
    "ds = pd.merge(text, summaries, on='url')[['article_content', 'summary']]\n",
    "ds = ds.rename(columns={'article_content': 'text'})\n",
    "ds.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = pd.read_feather(args.data_path).iloc[:args.subset]\n",
    "ds = ds[ds['summary'] != '']\n",
    "train_ds, test_ds = train_test_split(ds, test_size=args.test_pct, random_state=42)\n",
    "valid_ds, test_ds = train_test_split(test_ds, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass our data to the model in our fastai2 learner object we need a dataloader. To create a dataloader we need a Datasets object, batch size, and device type. To create a Datasets object, we have to pass a few things:\n",
    "- Our raw data which in our case is a Pandas dataframe\n",
    "- A list of transforms. Or to be more precise a list containing the list of transforms to perform on our inputs and a list of transforms to perform on our desired outputs. I've defined a transform below that encodes the text using the BART tokenizer. Mostly it will be the encodes class method that gets called by fastai2. However the decodes method can also be useful if you want to reverse the process.\n",
    "- We will also split our data into training and validation datasets here, using fastai2's RandomSplitter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1af22098774b35880bf05d14407d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ab6c3fdb554ebeb285ac2bf5aaa948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('bart-large-cnn', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm still exploring whether it is necessary to pass any of the masks and other ids manually or if it is handled for us. Any advice here would be much appreciated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform(Transform):\n",
    "    def __init__(self, tokenizer:PreTrainedTokenizer, column:string):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.column = column\n",
    "        \n",
    "    def encodes(self, inp):  \n",
    "        tokenized = self.tokenizer.batch_encode_plus(\n",
    "            [list(inp[self.column])],\n",
    "            max_length=args.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return TensorText(tokenized['input_ids']).squeeze()\n",
    "        \n",
    "    def decodes(self, encoded):\n",
    "        decoded = [\n",
    "            self.tokenizer.decode(\n",
    "                o, \n",
    "                skip_special_tokens=True, \n",
    "                clean_up_tokenization_spaces=False\n",
    "            ) for o in encoded\n",
    "        ]\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms = [DataTransform(tokenizer, column='text')]\n",
    "y_tfms = [DataTransform(tokenizer, column='summary')]\n",
    "dss = Datasets(\n",
    "    train_ds, \n",
    "    tfms=[x_tfms, y_tfms], \n",
    "    splits=RandomSplitter(valid_pct=0.1)(range(train_ds.shape[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dss.dataloaders(bs=args.batch_size, device=args.device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function lets us choose between loading the model architecture with Facebook's pretrained weights, the model architecture with our own weights stored locally, or the model architecture with no pretraining at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_model(config, pretrained=False, path=None): \n",
    "    if pretrained:    \n",
    "        if path:\n",
    "            model = BartForConditionalGeneration.from_pretrained(\n",
    "                \"bart-large-cnn\", \n",
    "                state_dict=torch.load(path, map_location=torch.device(args.device)), \n",
    "                config=config\n",
    "            )\n",
    "        else: \n",
    "            model = BartForConditionalGeneration.from_pretrained(\"bart-large-cnn\", config=config)\n",
    "    else:\n",
    "        model = BartForConditionalGeneration()\n",
    "\n",
    "    return model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will return a lot of different things, but we only want the weights to calculate the loss when training, so we will wrap the model in this class to control what gets passed to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastaiWrapper(Module):\n",
    "    def __init__(self):\n",
    "        self.config = BartConfig(vocab_size=50264, output_past=True)\n",
    "        self.bart = load_hf_model(config=self.config, pretrained=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.bart(x)[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of seq2seq tasks as a series of attempts to categorise which word should come next. Cross entropy loss is a pretty good loss function for this use case. We want to normalise it by how many non padding words are in each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarisationLoss(Module):\n",
    "    def __init__(self):\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        x = F.log_softmax(output, dim=-1)\n",
    "        norm = (target != 1).data.sum()\n",
    "        return self.criterion(x.contiguous().view(-1, x.size(-1)), target.contiguous().view(-1)) / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fine-tuning the model we start by just training the top layer(s). You can experiment by unfreezing layers further down in the decoder, and then (if you're feeling bold) then encoder. fastai2 provides an easy way to split the model up into groups with frozen or unfrozen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_splitter(model):\n",
    "    return [\n",
    "        params(model.bart.model.encoder), \n",
    "        params(model.bart.model.decoder.embed_tokens),\n",
    "        params(model.bart.model.decoder.embed_positions),\n",
    "        params(model.bart.model.decoder.layers),\n",
    "        params(model.bart.model.decoder.layernorm_embedding),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been experimenting with half precision training. In theory this will save a lot of memory. However, I find my loss quickly becomes a bunch of nans. This may be an issue with HuggingFace's implementation or it may be an issue with my code. I'll update if I work out how to get fp16() working. Do let me know if you have any ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls, \n",
    "    FastaiWrapper(), \n",
    "    loss_func=SummarisationLoss(), \n",
    "    opt_func=ranger,\n",
    "    splitter=bart_splitter\n",
    ")#.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been finding that the learning rate finder suggests values that are too high. Your mileage may vary though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_flat_cos(\n",
    "    1,\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do carry on unfreezing layers, you may find that you need to reduce your batch size to fit everything in memory. Also you should probably lower your learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.dls.train.bs = args.batch_size//2\n",
    "learn.dls.valid.bs = args.batch_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_flat_cos(\n",
    "    2,\n",
    "    lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is done we can export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('../models/fintuned_bart.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner('../models/fintuned_bart.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code for generating the summaries comes from [Sam Shleifer's example in the Transformers repository](https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/evaluate_cnn.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def generate_summaries(lns, out_file, batch_size=4):\n",
    "    dec = []\n",
    "    for batch in progress_bar(list(chunks(lns, batch_size))):\n",
    "        dct = tokenizer.batch_encode_plus(\n",
    "            batch, \n",
    "            max_length=1024, \n",
    "            return_tensors=\"pt\", \n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "        \n",
    "        summaries = learn.model.bart.to(args.device).generate(\n",
    "            input_ids=dct[\"input_ids\"].to(args.device),\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            max_length=142,\n",
    "            min_length=56,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "        \n",
    "        dec.extend([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries])\n",
    "        \n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2/2 00:08<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lns = [\" \" + x.rstrip() for x in list(test_ds['text'])[:8]]\n",
    "bart_sums = generate_summaries(lns, f'{args.stories_folder}/output.txt', batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPPO's first 5G smartphone has received 5G CE certification, paving the way for the company to commercially launch the device in Europe. The phone maker claims that it is the first multi-frequency, multi-mode and multi-EN-DC (which means dual connectivity for LTE and 5G) smartphone to be certified by CTC Advanced GmbH. OPPO inked a global patent license agreement with Ericsson, which covers the patent portfolios of both companies in 2G, 3G and 4G, as well as cooperation on device testing, customer engagements and a demonstration at MWC19.\n",
      "***************\n",
      "Streaming music from Amazon Music, TuneIn, iHeartRadio, and Pandora, with support for Spotify and SiriusXM available shortly. Using the Amazon Alexa App simply create groups of Echo devices and then simply ask Alexa to play on those devices. Amazon is excited to be working with leading brands on this offering, including Sonos, Bose, Sound United, and Samsung.\n",
      "***************\n",
      "LanzaTech uses anaerobic bacteria (originally found in rabbit droppings) to ferment waste emissions from industry. Facility has a capacity of 46,000 tons (16 million gallons) of ethanol per year. This ethanol’s performance in fuel blending applications is indistinguishable from sugar-derived ethanol. It meets all specifications of ASTM International D4806, the active standard for qualifying ethanol used in blending with gasoline for automotive engines. In addition, the ethanol meets the National Standard of the People's Republic of China GB 18350 for Denatured fuel ethanol.\n",
      "***************\n",
      "UK energy leaders are more worried about the risk of cyber attacks caused by the rise in digitisation of energy systems than other countries. The rapid expansion of solar, the smart meter roll-out and the introduction of half hourly metering has increased the level of digital reliance. Other key uncertainties that keep Britain's energy leaders awake at night were cohesion in the European Union post-Brexit, electric storage and commodity prices. Top action priorities were renewable energy, the global climate framework, energy efficiency and electricity prices.\n",
      "***************\n",
      "Coal use by utility companies has plummeted amid low natural gas prices. Ten years ago coal produced 50 percent of the nation’s power supply. California is the largest insurance market in the United States and sixth-largest in the world. Arch Coal Inc, the nation's second-largest U.S. coal miner, filed for bankruptcy protection earlier this month.\n",
      "***************\n",
      "China is building a vast DNA database with no appropriate privacy protection, rights activists warn. Ordinary citizens are being asked to have their blood drawn for a DNA sample, Human Rights Watch says. Vulnerable groups and minorities appear to be a particular target of the push. Xinjiang authorities are reported to have bought around $10bn (£7.7bn) in equipment to step up the collection and indexing of DNA.\n",
      "***************\n",
      "New rules ban people on any of the Balearic islands from renting out space in flats via websites such as Airbnb and Homeaway unless they first obtain a licence. They face fines of up to €400,000 if they break the law. Companies, including Airbnb, face a similar penalty for allowing clients to advertise without a valid licence number. “What the local government is doing is half-witted — this is a tourist island, nothing else makes money,” said Ray, a Briton who preferred not to give his surname.\n",
      "***************\n",
      "Suzlon Energy said it defaulted on a $172-million bond payment on Tuesday. Its shares fell more than 3% to close at Rs 4.6 on Tuesday on the BSE. Sources said the wind power equipment maker was in talks with several global private equity funds to sell a majority stake. Suzlon is struggling with debt of over Rs 11,000 crore ($1.6 billion)\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "for s in bart_sums[:8]:\n",
    "    print(s)\n",
    "    print(\"***************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "72631b56-0587-4b65-a58b-37713fb41f41"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
