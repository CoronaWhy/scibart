{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning BART for abstractive text summarisation with fastai2\n",
    "\n",
    "A great thing about working in NLP at the moment is being able to park a hard problem for a few weeks and discovering the community making massive amounts of progress on your behalf. I used to be overwhelmed by the challenge of just training a summarisation model to generate plausible looking text without burning through tonnes of cash on GPUs. Then [BertExtAbs](../finetuning-bertsumextabs) came along and solved that problem. Unfortunately, it still gernerated incoherent sentences sometimes and had a habit of confusing entities in an article. You certainly couldn't trust it to convey the facts of an article reliably.\n",
    "\n",
    "Enter BART (Bidirectional and Auto-Regressive Transformers). Here we have a model that generates staggeringly good summaries and has a wonderful implementation from Sam Shleifer at HuggingFace. It's still a work in progress, but after digging around in the Transformers pull requests and with help from [Morgan McGuire's FastHugs notebook](https://github.com/morganmcg1/fasthugs) I have put together this notebook for fine-tuning BART and generating summaries. Feedback welcome!\n",
    "\n",
    "I should mention that this a big model requiring big inputs. For fine-tuning I've been able to get a batch size of 4 and a maximum sequence length of 512 on an AWS P3.2xlarge (~Â£4 an hour).\n",
    "\n",
    "We begin with a bunch of imports and an args object for storing variables we will need. We'll be finetuning the model on the Curation Corpus of abstractive text summaries. We load it into a dataframe using Pandas. For more information about how to access this dataset for your own purposes please see our [article introducing the dataset](https://medium.com/curation-corporation/teaching-an-ai-to-abstract-a-new-dataset-for-abstractive-auto-summarisation-5227f546caa8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastai2 in /usr/local/lib/python3.7/site-packages (0.0.16)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/site-packages (2.8.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/site-packages (from fastai2) (7.0.0)\n",
      "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.7/site-packages (from fastai2) (0.5.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/site-packages (from fastai2) (0.21.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/site-packages (from fastai2) (1.4.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/site-packages (from fastai2) (3.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from fastai2) (2.21.0)\n",
      "Requirement already satisfied: fastprogress>=0.1.22 in /usr/local/lib/python3.7/site-packages (from fastai2) (0.2.3)\n",
      "Requirement already satisfied: fastcore in /usr/local/lib/python3.7/site-packages (from fastai2) (0.1.16)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (from fastai2) (3.13)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/site-packages (from fastai2) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from pandas) (1.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas) (2018.5)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/site-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.7/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/site-packages (from transformers) (1.9.111)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from torchvision>=0.5->fastai2) (1.13.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn->fastai2) (0.13.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->fastai2) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib->fastai2) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->fastai2) (2.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->fastai2) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->fastai2) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->fastai2) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->fastai2) (3.0.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (0.6.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (42.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (7.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from spacy->fastai2) (3.0.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.111 in /usr/local/lib/python3.7/site-packages (from boto3->transformers) (1.12.253)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.7/site-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai2) (1.6.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.111->boto3->transformers) (0.14)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai2) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# import sys  \n",
    "# !{sys.executable} -m pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from fastprogress import progress_bar\n",
    "from fastai2.basics import *\n",
    "from fastai2.data import *\n",
    "from fastai2.text.all import *\n",
    "from fastai2.callback.all import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import PreTrainedTokenizer, BartTokenizer, BartForConditionalGeneration, BartConfig \n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append('..')\n",
    "logging.getLogger().setLevel(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully we will be able to increase our batch size and/or maximum sequence lengths when some pull requests to reduce the model's memory footprint get merged into the Transformers repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "args = Namespace(\n",
    "    batch_size=4,\n",
    "    max_seq_len=512,\n",
    "    data_path=\"../data/private_dataset.file\",\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), # ('cpu'),\n",
    "    stories_folder='../data/my_own_stories',\n",
    "    subset=None,\n",
    "    test_pct=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset after scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showmult(*args):\n",
    "    return [i for i in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('../data/curation_corpus'),\n",
       " Path('../data/SemanticScholarAbstractSectionSummaryDataSet'),\n",
       " Path('../data/ArxivStructuredAbstractSectionalSummaries'),\n",
       " Path('../data/wikihow')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = glob.glob('../data/*/', recursive=True)\n",
    "paths = [Path(p) for p in paths]\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing curation_corpus\n",
      "curation_corpus done\n",
      "processing SemanticScholarAbstractSectionSummaryDataSet\n",
      "SemanticScholarAbstractSectionSummaryDataSet done\n",
      "processing ArxivStructuredAbstractSectionalSummaries\n",
      "ArxivStructuredAbstractSectionalSummaries done\n",
      "processing wikihow\n",
      "wikihow done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>data_src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit: CC0 Public Domain\\n             \\n\\nUniversity of British Columbia researchers have found a cheap, sustainable way to build a solar cell using bacteria that convert light to energy.\\n                                              \\nTheir cell generated a current stronger than any previously recorded from such a device, and worked as efficiently in dim light as in bright light.\\nThis innovation could be a step toward wider adoption of solar power in places like British Columbia and parts of northern Europe where overcast skies are common. With further development, these solar cellsâc...</td>\n",
       "      <td>Researchers in Canada have developed an innovative solar cell which uses bacteria to convert light into energy. The team at the University of British Columbia developed a way of genetically engineering E.coli to produce large amounts of lycopene, a natural dye which bacteria use for photosynthesis. By coating the bacteria with a semiconducting substance and incorporating it into a battery cell, they were able to achieve a current density of 0.686 milliamps per square centimetre, which they say is the highest yet achieved by a biogenic solar cell.</td>\n",
       "      <td>curation_corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Genesis Motor America is launching two new online tools as part of its newly redesigned website.In conjunction with the debut of the Genesis G70, AOR Innocean took gaming technologies and\\nInstagram Stories and created two new automotive \"configurators.\"The first, a web-based Automotive Real-Time 3D Configurator, can only be found on the updated website, Genesis.com. Here, users can build a virtual vehicle while exploring inside and outside the car, activating animations that emulate real vehicle actions including\\nheadlights, sunroof, and dashboard. They can even pop the trunk open. Users...</td>\n",
       "      <td>Luxury car maker Genesis Motor America launched aÂ redesigned website that included two interactive tools that enable users to build a virtual vehicle complete with animations that mimic functions such as opening the sunroof. One of the tools allows Instagram users to build their car on the app using Instagram Stories. Innocean lead the project handling the creative and concept design. MediaMonks was one of four other companies that participated on the project, working on the WebGL Game Engine.\\n</td>\n",
       "      <td>curation_corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When Jamie Hodari started looking for funding for his New York-based co-working startup, Industrious, in 2012, he didnât even bother talking to venture capital firms. \\nâWe were just so confident that VCs didnât fund real estate that it wasnât worth trying,â he said. \\nInstead, Hodari and his co-founder, Justin Stewart, went to anyone they knew â parents, siblings, aunts, uncles, friends â who had money and pitched them on investing in individual locations. While they managed to raise $8 million, it was tedious. They couldnât sign a lease or get going on a project until they had every dime...</td>\n",
       "      <td>Real-estate tech has become hot property for VC funds. Struggling to find seed funding when it launched in 2012, coworkspace operator Industrious was forced to tap friends and family for $8m. By early 2018, two rounds later, it had raised a total of $142m and had a list of 64 funds keen to invest in the sector. Industrious is one of hundredsÂ benefiting from the explosion of interest in real-estate venture investment. From 2012 to 2017, investment in US real-estate tech leapt from $44.7m to $5.7bn, according to Pitchbook,Â as VC funds exclusively devoted to the sector emerged and family real...</td>\n",
       "      <td>curation_corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>British Insurers have today highlighted the potential dangers of âautonomous ambiguityâ, as vehicles with different levels of autonomy, or driverless technology, increasingly become a feature of UK roads.With important and wide-reaching changes being defined by international regulators on what Assisted and Automated systems can and can't do, the Automated Driving Insurer Group (ADIG), led by the Association of British Insurers (ABI) in collaboration with Thatcham Research, has released a white paper setting out the latest position of UK insurers.The âRegulating Automated Drivingâ paper is ...</td>\n",
       "      <td>While they believe vehicle automation will significantly reduce accidents, British insurers, including esure, have voiced concerns over âautonomous ambiguityâÂ as vehicles with different levels of autonomy take to the roads. The insurers have called for international regulators to make clear distinctions between assisted and automated systems, and setÂ out criteria for marketing such vehicles. It is suggested that \"intermediate automated systems\", which offer significant self-driving capability but require drivers to reclaim control of the vehicle in certain circumstances, could leave driver...</td>\n",
       "      <td>curation_corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sir Martin Sorrell, the former boss of advertising giant WPP, is poised to wrap up a second takeover at his new venture this week.Sorrell, 73, who dramatically quit WPP following a board investigation into his conduct, is now running S4 Capital, a vehicle he set up to acquire marketing and advertising businesses.S4 is likely to confirm a takeover of US programmatic ad firm MightyHive in the coming days, The Mail on Sunday understands. The firm has been valued at up to $200 million (Â£157 million). Pastures new: Sir Martin Sorrell dramatically quit WPP following a board investigation into hi...</td>\n",
       "      <td>S4 Capital is close to acquiring MightyHive, with a deal expected within the next few days, according to The Mail on Sunday. The deal would be S4 Capitalâs second acquisition since it was founded earlier this year; it beat WPP in a bidding war forÂ MediaMonks in June.Â Â US programmatic ad firm MightyHive has been valued at up to $200m.\\n</td>\n",
       "      <td>curation_corpus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  Credit: CC0 Public Domain\\n             \\n\\nUniversity of British Columbia researchers have found a cheap, sustainable way to build a solar cell using bacteria that convert light to energy.\\n                                              \\nTheir cell generated a current stronger than any previously recorded from such a device, and worked as efficiently in dim light as in bright light.\\nThis innovation could be a step toward wider adoption of solar power in places like British Columbia and parts of northern Europe where overcast skies are common. With further development, these solar cellsâc...   \n",
       "1  Genesis Motor America is launching two new online tools as part of its newly redesigned website.In conjunction with the debut of the Genesis G70, AOR Innocean took gaming technologies and\\nInstagram Stories and created two new automotive \"configurators.\"The first, a web-based Automotive Real-Time 3D Configurator, can only be found on the updated website, Genesis.com. Here, users can build a virtual vehicle while exploring inside and outside the car, activating animations that emulate real vehicle actions including\\nheadlights, sunroof, and dashboard. They can even pop the trunk open. Users...   \n",
       "2  When Jamie Hodari started looking for funding for his New York-based co-working startup, Industrious, in 2012, he didnât even bother talking to venture capital firms. \\nâWe were just so confident that VCs didnât fund real estate that it wasnât worth trying,â he said. \\nInstead, Hodari and his co-founder, Justin Stewart, went to anyone they knew â parents, siblings, aunts, uncles, friends â who had money and pitched them on investing in individual locations. While they managed to raise $8 million, it was tedious. They couldnât sign a lease or get going on a project until they had every dime...   \n",
       "3  British Insurers have today highlighted the potential dangers of âautonomous ambiguityâ, as vehicles with different levels of autonomy, or driverless technology, increasingly become a feature of UK roads.With important and wide-reaching changes being defined by international regulators on what Assisted and Automated systems can and can't do, the Automated Driving Insurer Group (ADIG), led by the Association of British Insurers (ABI) in collaboration with Thatcham Research, has released a white paper setting out the latest position of UK insurers.The âRegulating Automated Drivingâ paper is ...   \n",
       "4  Sir Martin Sorrell, the former boss of advertising giant WPP, is poised to wrap up a second takeover at his new venture this week.Sorrell, 73, who dramatically quit WPP following a board investigation into his conduct, is now running S4 Capital, a vehicle he set up to acquire marketing and advertising businesses.S4 is likely to confirm a takeover of US programmatic ad firm MightyHive in the coming days, The Mail on Sunday understands. The firm has been valued at up to $200 million (Â£157 million). Pastures new: Sir Martin Sorrell dramatically quit WPP following a board investigation into hi...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   summary  \\\n",
       "0                                                 Researchers in Canada have developed an innovative solar cell which uses bacteria to convert light into energy. The team at the University of British Columbia developed a way of genetically engineering E.coli to produce large amounts of lycopene, a natural dye which bacteria use for photosynthesis. By coating the bacteria with a semiconducting substance and incorporating it into a battery cell, they were able to achieve a current density of 0.686 milliamps per square centimetre, which they say is the highest yet achieved by a biogenic solar cell.   \n",
       "1                                                                                                     Luxury car maker Genesis Motor America launched aÂ redesigned website that included two interactive tools that enable users to build a virtual vehicle complete with animations that mimic functions such as opening the sunroof. One of the tools allows Instagram users to build their car on the app using Instagram Stories. Innocean lead the project handling the creative and concept design. MediaMonks was one of four other companies that participated on the project, working on the WebGL Game Engine.\\n   \n",
       "2  Real-estate tech has become hot property for VC funds. Struggling to find seed funding when it launched in 2012, coworkspace operator Industrious was forced to tap friends and family for $8m. By early 2018, two rounds later, it had raised a total of $142m and had a list of 64 funds keen to invest in the sector. Industrious is one of hundredsÂ benefiting from the explosion of interest in real-estate venture investment. From 2012 to 2017, investment in US real-estate tech leapt from $44.7m to $5.7bn, according to Pitchbook,Â as VC funds exclusively devoted to the sector emerged and family real...   \n",
       "3  While they believe vehicle automation will significantly reduce accidents, British insurers, including esure, have voiced concerns over âautonomous ambiguityâÂ as vehicles with different levels of autonomy take to the roads. The insurers have called for international regulators to make clear distinctions between assisted and automated systems, and setÂ out criteria for marketing such vehicles. It is suggested that \"intermediate automated systems\", which offer significant self-driving capability but require drivers to reclaim control of the vehicle in certain circumstances, could leave driver...   \n",
       "4                                                                                                                                                                                                                                                                        S4 Capital is close to acquiring MightyHive, with a deal expected within the next few days, according to The Mail on Sunday. The deal would be S4 Capitalâs second acquisition since it was founded earlier this year; it beat WPP in a bidding war forÂ MediaMonks in June.Â Â US programmatic ad firm MightyHive has been valued at up to $200m.\\n   \n",
       "\n",
       "          data_src  \n",
       "0  curation_corpus  \n",
       "1  curation_corpus  \n",
       "2  curation_corpus  \n",
       "3  curation_corpus  \n",
       "4  curation_corpus  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = glob.glob('../raw_data/*/', recursive=True)\n",
    "paths = [Path(p) for p in paths]\n",
    "\n",
    "# read in datasets. use more of a dict approach here\n",
    "cols2keep = {\n",
    "    'SemanticScholarAbstractSectionSummaryDataSet': ['paperSection', 'Summary'],\n",
    "    'ArxivStructuredAbstractSectionalSummaries': ['paperSection', 'Summary'],\n",
    "    'wikihow': ['text', 'overview'],\n",
    "    'curation_corpus': ['article_content', 'summary'],  # renames 'article_content' column as 'text'\n",
    "}\n",
    "\n",
    "output_dir = Path('../data')\n",
    "\n",
    "\n",
    "# appends everything to the same dataframe, so far that's managable\n",
    "data = []\n",
    "for p in paths:\n",
    "\n",
    "    name = os.path.split(p)[-1]\n",
    "\n",
    "    print(f\"processing {name}\")\n",
    "\n",
    "    # read files\n",
    "    files = p.iterdir()\n",
    "        \n",
    "    # wrangle curation corpus\n",
    "    if 'curation_corpus' in name:\n",
    "        parent_dir = list(files)[0].parent\n",
    "        summaries = pd.read_csv(parent_dir / 'curation-corpus-base.csv')\n",
    "        text = pd.read_csv(parent_dir / 'curation-corpus-base-with-articles.csv')\n",
    "        text = text[text.html != 'Exception']\n",
    "        df = pd.merge(text, summaries, on='url')[['article_content', 'summary']]\n",
    "\n",
    "    # read wikihow data\n",
    "    elif 'wikihow' in name:\n",
    "        df = pd.read_csv(list(files)[0])\n",
    "\n",
    "    # read everything else\n",
    "    else:\n",
    "        dfs = [\n",
    "            pd.read_parquet(f) for f in files\n",
    "            if 'parquet' in str(f)\n",
    "        ]\n",
    "        df = pd.concat(dfs)\n",
    "\n",
    "    df = df[cols2keep[name]]\n",
    "    df.columns = ['text', 'summary']\n",
    "    df['data_src'] = name\n",
    "    data.append(df)\n",
    "\n",
    "    print(f\"{name} done\")\n",
    "\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92485d4f9c694bdcb32feb622743f189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=40.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path('../data')\n",
    "\n",
    "nchunks = 40\n",
    "datas = np.array_split(data, nchunks)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for ix, d in enumerate(tqdm(datas)):\n",
    "    d.to_parquet(output_dir / f'data{str(ix).zfill(2)}.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = pd.read_feather(args.data_path).iloc[:args.subset]\n",
    "ds = data\n",
    "ds = ds[ds['summary'] != '']\n",
    "train_ds, test_ds = train_test_split(ds, test_size=args.test_pct, random_state=42)\n",
    "valid_ds, test_ds = train_test_split(test_ds, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass our data to the model in our fastai2 learner object we need a dataloader. To create a dataloader we need a Datasets object, batch size, and device type. To create a Datasets object, we have to pass a few things:\n",
    "- Our raw data which in our case is a Pandas dataframe\n",
    "- A list of transforms. Or to be more precise a list containing the list of transforms to perform on our inputs and a list of transforms to perform on our desired outputs. I've defined a transform below that encodes the text using the BART tokenizer. Mostly it will be the encodes class method that gets called by fastai2. However the decodes method can also be useful if you want to reverse the process.\n",
    "- We will also split our data into training and validation datasets here, using fastai2's RandomSplitter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1af22098774b35880bf05d14407d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descriptiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ab6c3fdb554ebeb285ac2bf5aaa948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('bart-large-cnn', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm still exploring whether it is necessary to pass any of the masks and other ids manually or if it is handled for us. Any advice here would be much appreciated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform(Transform):\n",
    "    def __init__(self, tokenizer:PreTrainedTokenizer, column:string):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.column = column\n",
    "        \n",
    "    def encodes(self, inp):  \n",
    "        tokenized = self.tokenizer.batch_encode_plus(\n",
    "            [list(inp[self.column])],\n",
    "            max_length=args.max_seq_len, \n",
    "            pad_to_max_length=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return TensorText(tokenized['input_ids']).squeeze()\n",
    "        \n",
    "    def decodes(self, encoded):\n",
    "        decoded = [\n",
    "            self.tokenizer.decode(\n",
    "                o, \n",
    "                skip_special_tokens=True, \n",
    "                clean_up_tokenization_spaces=False\n",
    "            ) for o in encoded\n",
    "        ]\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms = [DataTransform(tokenizer, column='text')]\n",
    "y_tfms = [DataTransform(tokenizer, column='summary')]\n",
    "dss = Datasets(\n",
    "    train_ds, \n",
    "    tfms=[x_tfms, y_tfms], \n",
    "    splits=RandomSplitter(valid_pct=0.1)(range(train_ds.shape[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dss.dataloaders(bs=args.batch_size, device=args.device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function lets us choose between loading the model architecture with Facebook's pretrained weights, the model architecture with our own weights stored locally, or the model architecture with no pretraining at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_model(config, pretrained=False, path=None): \n",
    "    if pretrained:    \n",
    "        if path:\n",
    "            model = BartForConditionalGeneration.from_pretrained(\n",
    "                \"bart-large-cnn\", \n",
    "                state_dict=torch.load(path, map_location=torch.device(args.device)), \n",
    "                config=config\n",
    "            )\n",
    "        else: \n",
    "            model = BartForConditionalGeneration.from_pretrained(\"bart-large-cnn\", config=config)\n",
    "    else:\n",
    "        model = BartForConditionalGeneration()\n",
    "\n",
    "    return model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will return a lot of different things, but we only want the weights to calculate the loss when training, so we will wrap the model in this class to control what gets passed to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastaiWrapper(Module):\n",
    "    def __init__(self):\n",
    "        self.config = BartConfig(vocab_size=50264, output_past=True)\n",
    "        self.bart = load_hf_model(config=self.config, pretrained=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.bart(x)[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of seq2seq tasks as a series of attempts to categorise which word should come next. Cross entropy loss is a pretty good loss function for this use case. We want to normalise it by how many non padding words are in each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarisationLoss(Module):\n",
    "    def __init__(self):\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        x = F.log_softmax(output, dim=-1)\n",
    "        norm = (target != 1).data.sum()\n",
    "        return self.criterion(x.contiguous().view(-1, x.size(-1)), target.contiguous().view(-1)) / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fine-tuning the model we start by just training the top layer(s). You can experiment by unfreezing layers further down in the decoder, and then (if you're feeling bold) then encoder. fastai2 provides an easy way to split the model up into groups with frozen or unfrozen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_splitter(model):\n",
    "    return [\n",
    "        params(model.bart.model.encoder), \n",
    "        params(model.bart.model.decoder.embed_tokens),\n",
    "        params(model.bart.model.decoder.embed_positions),\n",
    "        params(model.bart.model.decoder.layers),\n",
    "        params(model.bart.model.decoder.layernorm_embedding),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been experimenting with half precision training. In theory this will save a lot of memory. However, I find my loss quickly becomes a bunch of nans. This may be an issue with HuggingFace's implementation or it may be an issue with my code. I'll update if I work out how to get fp16() working. Do let me know if you have any ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls, \n",
    "    FastaiWrapper(), \n",
    "    loss_func=SummarisationLoss(), \n",
    "    opt_func=ranger,\n",
    "    splitter=bart_splitter\n",
    ")#.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been finding that the learning rate finder suggests values that are too high. Your mileage may vary though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_flat_cos(\n",
    "    1,\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do carry on unfreezing layers, you may find that you need to reduce your batch size to fit everything in memory. Also you should probably lower your learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.dls.train.bs = args.batch_size//2\n",
    "learn.dls.valid.bs = args.batch_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_flat_cos(\n",
    "    2,\n",
    "    lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is done we can export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('../models/fintuned_bart.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner('../models/fintuned_bart.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code for generating the summaries comes from [Sam Shleifer's example in the Transformers repository](https://github.com/huggingface/transformers/blob/master/examples/summarization/bart/evaluate_cnn.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def generate_summaries(lns, out_file, batch_size=4):\n",
    "    dec = []\n",
    "    for batch in progress_bar(list(chunks(lns, batch_size))):\n",
    "        dct = tokenizer.batch_encode_plus(\n",
    "            batch, \n",
    "            max_length=1024, \n",
    "            return_tensors=\"pt\", \n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "        \n",
    "        summaries = learn.model.bart.to(args.device).generate(\n",
    "            input_ids=dct[\"input_ids\"].to(args.device),\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            max_length=142,\n",
    "            min_length=56,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "        \n",
    "        dec.extend([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries])\n",
    "        \n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2/2 00:08<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lns = [\" \" + x.rstrip() for x in list(test_ds['text'])[:8]]\n",
    "bart_sums = generate_summaries(lns, f'{args.stories_folder}/output.txt', batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPPO's first 5G smartphone has received 5G CE certification, paving the way for the company to commercially launch the device in Europe. The phone maker claims that it is the first multi-frequency, multi-mode and multi-EN-DC (which means dual connectivity for LTE and 5G) smartphone to be certified by CTC Advanced GmbH. OPPO inked a global patent license agreement with Ericsson, which covers the patent portfolios of both companies in 2G, 3G and 4G, as well as cooperation on device testing, customer engagements and a demonstration at MWC19.\n",
      "***************\n",
      "Streaming music from Amazon Music, TuneIn, iHeartRadio, and Pandora, with support for Spotify and SiriusXM available shortly. Using the Amazon Alexa App simply create groups of Echo devices and then simply ask Alexa to play on those devices. Amazon is excited to be working with leading brands on this offering, including Sonos, Bose, Sound United, and Samsung.\n",
      "***************\n",
      "LanzaTech uses anaerobic bacteria (originally found in rabbit droppings) to ferment waste emissions from industry. Facility has a capacity of 46,000 tons (16 million gallons) of ethanol per year. This ethanolâs performance in fuel blending applications is indistinguishable from sugar-derived ethanol. It meets all specifications of ASTM International D4806, the active standard for qualifying ethanol used in blending with gasoline for automotive engines. In addition, the ethanol meets the National Standard of the People's Republic of China GB 18350 for Denatured fuel ethanol.\n",
      "***************\n",
      "UK energy leaders are more worried about the risk of cyber attacks caused by the rise in digitisation of energy systems than other countries. The rapid expansion of solar, the smart meter roll-out and the introduction of half hourly metering has increased the level of digital reliance. Other key uncertainties that keep Britain's energy leaders awake at night were cohesion in the European Union post-Brexit, electric storage and commodity prices. Top action priorities were renewable energy, the global climate framework, energy efficiency and electricity prices.\n",
      "***************\n",
      "Coal use by utility companies has plummeted amid low natural gas prices. Ten years ago coal produced 50 percent of the nationâs power supply. California is the largest insurance market in the United States and sixth-largest in the world. Arch Coal Inc, the nation's second-largest U.S. coal miner, filed for bankruptcy protection earlier this month.\n",
      "***************\n",
      "China is building a vast DNA database with no appropriate privacy protection, rights activists warn. Ordinary citizens are being asked to have their blood drawn for a DNA sample, Human Rights Watch says. Vulnerable groups and minorities appear to be a particular target of the push. Xinjiang authorities are reported to have bought around $10bn (Â£7.7bn) in equipment to step up the collection and indexing of DNA.\n",
      "***************\n",
      "New rules ban people on any of the Balearic islands from renting out space in flats via websites such as Airbnb and Homeaway unless they first obtain a licence. They face fines of up to â¬400,000 if they break the law. Companies, including Airbnb, face a similar penalty for allowing clients to advertise without a valid licence number. âWhat the local government is doing is half-witted â this is a tourist island, nothing else makes money,â said Ray, a Briton who preferred not to give his surname.\n",
      "***************\n",
      "Suzlon Energy said it defaulted on a $172-million bond payment on Tuesday. Its shares fell more than 3% to close at Rs 4.6 on Tuesday on the BSE. Sources said the wind power equipment maker was in talks with several global private equity funds to sell a majority stake. Suzlon is struggling with debt of over Rs 11,000 crore ($1.6 billion)\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "for s in bart_sums[:8]:\n",
    "    print(s)\n",
    "    print(\"***************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "72631b56-0587-4b65-a58b-37713fb41f41"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
